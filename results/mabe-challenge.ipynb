{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle.ipynb — 用已保存的模型在 Kaggle 上推理并生成 submission.csv\n",
    "\n",
    "本 Notebook 默认按 **LightGBM（本地 `local.ipynb` 训练产物）** 进行推理，并自动生成 `/kaggle/working/submission.csv`。\n",
    "\n",
    "## 你需要上传哪些内容（强烈建议做成一个 Kaggle Dataset）\n",
    "把本地目录下的以下文件上传到 Kaggle，方式建议：**Create Dataset → Upload**，然后在 Notebook 里 **Add Data**。\n",
    "\n",
    "必须：\n",
    "- `models/` 目录（包含多个 `lgbm_{action}.txt`）\n",
    "- `thresholds.joblib`\n",
    "\n",
    "建议（可选）：\n",
    "- `train_frame_samples_cfg.joblib`（仅用于记录超参；推理不需要）\n",
    "\n",
    "### 你需要修改哪里\n",
    "只需要改第 1 个代码单元中的：\n",
    "- `ARTIFACT_DATASET_NAME`：你上传的模型数据集目录名（出现在 `/kaggle/input/<这里>`）\n",
    "\n",
    "竞赛数据路径会自动探测（通过查找 `test.csv`），一般不需要改。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T12:39:47.983450Z",
     "iopub.status.busy": "2025-12-13T12:39:47.983060Z",
     "iopub.status.idle": "2025-12-13T12:39:47.991722Z",
     "shell.execute_reply": "2025-12-13T12:39:47.990754Z",
     "shell.execute_reply.started": "2025-12-13T12:39:47.983426Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_TYPE: lgbm\n",
      "WORK_DIR: /kaggle/working\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "# === 你要改的基本只有这一项 ===\n",
    "# 这是你在 Kaggle 上传的“模型产物 Dataset”在 /kaggle/input 下出现的目录名\n",
    "# 例：如果 /kaggle/input 里有文件夹 \"mabe-lgbm-artifacts\"，这里就填这个\n",
    "ARTIFACT_DATASET_NAME = \"mabe-challenge-dataset\"\n",
    "\n",
    "# 模型类型：默认 lgbm（本项目实际使用）。\n",
    "# 下面也给了 pytorch/tensorflow/onnx 的模板（可不管）。\n",
    "MODEL_TYPE = \"lgbm\"  # {\"lgbm\",\"pytorch\",\"tensorflow\",\"onnx\"}\n",
    "\n",
    "# 竞赛数据：一般无需改，Notebook 会自动扫描 /kaggle/input 找到含 test.csv 的目录\n",
    "COMP_DATA_DIR: Path | None = None\n",
    "\n",
    "# 产物输入目录（模型/阈值所在）\n",
    "ARTIFACT_DIR: Path | None = None\n",
    "\n",
    "# 输出目录（Kaggle 要求写到 /kaggle/working）\n",
    "WORK_DIR = Path(\"/kaggle/working\")\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 推理参数（通常无需改）\n",
    "MEDIAN_KERNEL = 9       # odd\n",
    "MIN_DURATION_SEC = 0.10\n",
    "MERGE_GAP_SEC = 0.05\n",
    "OUTPUT_SELF_AS_LITERAL = True\n",
    "\n",
    "# 提交列名（比赛格式固定，一般无需改）\n",
    "SUB_COLS = [\"row_id\", \"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "\n",
    "assert MODEL_TYPE in {\"lgbm\", \"pytorch\", \"tensorflow\", \"onnx\"}\n",
    "\n",
    "print(\"MODEL_TYPE:\", MODEL_TYPE)\n",
    "print(\"WORK_DIR:\", WORK_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T12:39:47.994012Z",
     "iopub.status.busy": "2025-12-13T12:39:47.993673Z",
     "iopub.status.idle": "2025-12-13T12:39:48.542453Z",
     "shell.execute_reply": "2025-12-13T12:39:48.541300Z",
     "shell.execute_reply.started": "2025-12-13T12:39:47.993987Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected COMP_DATA_DIR: /kaggle/input/MABe-mouse-behavior-detection\n",
      "ARTIFACT_DIR: /kaggle/input/mabe-challenge-dataset\n",
      "\n",
      "Available /kaggle/input dirs:\n",
      "- MABe-mouse-behavior-detection\n",
      "- mabe-challenge-dataset\n",
      "\n",
      "[tree] /kaggle/input/MABe-mouse-behavior-detection\n",
      "- test_tracking/\n",
      "  - test_tracking/AdaptableSnail/\n",
      "    - test_tracking/AdaptableSnail/438887472.parquet\n",
      "- train_annotation/\n",
      "  - train_annotation/AdaptableSnail/\n",
      "    - train_annotation/AdaptableSnail/1212811043.parquet\n",
      "    - train_annotation/AdaptableSnail/1260392287.parquet\n",
      "    - train_annotation/AdaptableSnail/1351098077.parquet\n",
      "    - train_annotation/AdaptableSnail/1408652858.parquet\n",
      "    - train_annotation/AdaptableSnail/143861384.parquet\n",
      "    - train_annotation/AdaptableSnail/1596473327.parquet\n",
      "    - train_annotation/AdaptableSnail/1643942986.parquet\n",
      "    - train_annotation/AdaptableSnail/1717182687.parquet\n",
      "    - train_annotation/AdaptableSnail/2078515636.parquet\n",
      "    - train_annotation/AdaptableSnail/209576908.parquet\n",
      "    - train_annotation/AdaptableSnail/278643799.parquet\n",
      "    - train_annotation/AdaptableSnail/351967631.parquet\n",
      "    - train_annotation/AdaptableSnail/355542626.parquet\n",
      "    - train_annotation/AdaptableSnail/44566106.parquet\n",
      "    - train_annotation/AdaptableSnail/678426900.parquet\n",
      "    - train_annotation/AdaptableSnail/705948978.parquet\n",
      "    - train_annotation/AdaptableSnail/878123481.parquet\n",
      "  - train_annotation/BoisterousParrot/\n",
      "    - train_annotation/BoisterousParrot/1059582964.parquet\n",
      "    - train_annotation/BoisterousParrot/1184291605.parquet\n",
      "    - train_annotation/BoisterousParrot/1201849558.parquet\n",
      "    - train_annotation/BoisterousParrot/1459695188.parquet\n",
      "    - train_annotation/BoisterousParrot/1985626297.parquet\n",
      "    - train_annotation/BoisterousParrot/402963089.parquet\n",
      "    - train_annotation/BoisterousParrot/459610814.parquet\n",
      "    - train_annotation/BoisterousParrot/613246188.parquet\n",
      "  - train_annotation/CRIM13/\n",
      "    - train_annotation/CRIM13/1009459450.parquet\n",
      "    - train_annotation/CRIM13/1057221056.parquet\n",
      "    - train_annotation/CRIM13/1149348188.parquet\n",
      "    - train_annotation/CRIM13/1213233769.parquet\n",
      "    - train_annotation/CRIM13/1313797424.parquet\n",
      "    - train_annotation/CRIM13/1319309143.parquet\n",
      "    - train_annotation/CRIM13/1381910096.parquet\n",
      "    - train_annotation/CRIM13/1490000283.parquet\n",
      "    - train_annotation/CRIM13/1592386054.parquet\n",
      "    - train_annotation/CRIM13/1695807459.parquet\n",
      "    - train_annotation/CRIM13/1733903958.parquet\n",
      "    - train_annotation/CRIM13/1763467574.parquet\n",
      "    - train_annotation/CRIM13/1836572955.parquet\n",
      "    - train_annotation/CRIM13/1846302042.parquet\n",
      "    - train_annotation/CRIM13/1976539996.parquet\n",
      "    - train_annotation/CRIM13/2134094751.parquet\n",
      "    - train_annotation/CRIM13/363958890.parquet\n",
      "    - train_annotation/CRIM13/415181540.parquet\n",
      "    - train_annotation/CRIM13/670907179.parquet\n",
      "    - train_annotation/CRIM13/793202924.parquet\n",
      "    - train_annotation/CRIM13/840324395.parquet\n",
      "  - train_annotation/CalMS21_supplemental/\n",
      "    - train_annotation/CalMS21_supplemental/1006083669.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1012566686.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1012566850.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1013428301.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1014350333.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1015291592.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1017236040.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1019240807.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1032773418.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1039672228.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1047608795.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1082445110.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1085810295.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1088021251.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1092798869.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1094348590.parquet\n",
      "    - train_annotation/CalMS21_supplemental/109675929.parquet\n",
      "    - train_annotation/CalMS21_supplemental/110342188.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1105042798.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1110611032.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1116444590.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1132476818.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1133807709.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1138789923.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1142036064.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1149399074.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1158341062.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1161382388.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1161513440.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1180251359.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1185620508.parquet\n",
      "    - train_annotation/CalMS21_supplemental/119456810.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1197353862.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1207553838.parquet\n",
      "    - train_annotation/CalMS21_supplemental/121783441.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1231968667.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1267508818.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1272568731.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1272974030.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1277377564.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1278319432.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1293027619.parquet\n",
      "    - train_annotation/CalMS21_supplemental/129371066.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1301288495.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1320201420.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1327458782.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1349102959.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1356723283.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1357765938.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1362255317.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1369858702.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1383011506.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1390249533.parquet\n",
      "    - train_annotation/CalMS21_supplemental/139083951.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1411405064.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1431927054.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1432635091.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1433266993.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1442492479.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1450277210.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1458368239.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1465225735.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1465845118.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1474034537.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1480402902.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1491960718.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1498476778.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1500887132.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1501429415.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1505228483.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1508568688.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1512101591.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1516524771.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1520746939.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1523817911.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1525217575.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1531311092.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1536671381.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1542354568.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1543288869.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1573791367.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1576527395.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1580520873.parquet\n",
      "    - train_annotation/CalMS21_supplemental/158696718.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1591554758.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1591747692.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1591757847.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1600524631.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1602849635.parquet\n",
      "    - train_annotation/CalMS21_supplemental/162065037.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1622496449.parquet\n",
      "    - train_annotation/CalMS21_supplemental/162657728.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1630924702.parquet\n",
      "    - train_annotation/CalMS21_supplemental/163541934.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1639493039.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1639810716.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1644423633.parquet\n",
      "    - train_annotation/CalMS21_supplemental/165039922.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1652683860.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1658817801.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1675950633.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1683673468.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1691279417.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1691760122.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1692251795.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1695342547.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1695348554.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1703310497.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1706585169.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1724547877.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1727431909.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1729232290.parquet\n",
      "    - train_annotation/CalMS21_supplemental/17308182.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1733995999.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1741457167.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1743317707.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1744212563.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1761366259.parquet\n",
      "    - train_annotation/CalMS21_supplemental/176648153.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1771120435.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1774554697.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1776748589.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1788436101.parquet\n",
      "    - train_annotation/CalMS21_supplemental/182143115.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1821740904.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1828414810.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1830370330.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1831993765.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1853523159.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1865009668.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1869967336.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1870526030.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1872766812.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1875246754.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1881725588.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1887057159.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1887340589.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1900472420.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1907533650.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1910565976.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1914240174.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1915789767.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1917012134.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1952344815.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1954031391.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1962858066.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1970018431.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1980223276.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1984570060.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1988640923.parquet\n",
      "    - train_annotation/CalMS21_supplemental/199044781.parquet\n",
      "    - train_annotation/CalMS21_supplemental/1991863389.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2010437126.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2017225429.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2030380897.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2032102979.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2034534324.parquet\n",
      "    - train_annotation/CalMS21_supplemental/204814054.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2050459549.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2057608448.parquet\n",
      "    - train_annotation/CalMS21_supplemental/206411405.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2071350962.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2084840600.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2103676486.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2109429295.parquet\n",
      "    - train_annotation/CalMS21_supplemental/211473435.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2118799217.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2121861703.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2122787540.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2131370806.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2137179735.parquet\n",
      "    - train_annotation/CalMS21_supplemental/2143302778.parquet\n",
      "    - train_annotation/CalMS21_supplemental/229613936.parquet\n",
      "    - train_annotation/CalMS21_supplemental/247581397.parquet\n",
      "    - train_annotation/CalMS21_supplemental/257443481.parquet\n",
      "    - train_annotation/CalMS21_supplemental/270605934.parquet\n",
      "    - train_annotation/CalMS21_supplemental/283078835.parquet\n",
      "    - train_annotation/CalMS21_supplemental/287957149.parquet\n",
      "    - train_annotation/CalMS21_supplemental/29204526.parquet\n",
      "    - train_annotation/CalMS21_supplemental/292722123.parquet\n",
      "    - train_annotation/CalMS21_supplemental/293032904.parquet\n",
      "    - train_annotation/CalMS21_supplemental/293971737.parquet\n",
      "    - train_annotation/CalMS21_supplemental/29829833.parquet\n",
      "    - train_annotation/CalMS21_supplemental/298840331.parquet\n",
      "    - train_annotation/CalMS21_supplemental/304455631.parquet\n",
      "    - train_annotation/CalMS21_supplemental/311944917.parquet\n",
      "    - train_annotation/CalMS21_supplemental/31444359.parquet\n",
      "    - train_annotation/CalMS21_supplemental/318716558.parquet\n",
      "    - train_annotation/CalMS21_supplemental/320652992.parquet\n",
      "    - train_annotation/CalMS21_supplemental/321538526.parquet\n",
      "    - train_annotation/CalMS21_supplemental/324704866.parquet\n",
      "    - train_annotation/CalMS21_supplemental/35021334.parquet\n",
      "    - train_annotation/CalMS21_supplemental/352625654.parquet\n",
      "    - train_annotation/CalMS21_supplemental/358516464.parquet\n",
      "    - train_annotation/CalMS21_supplemental/379027684.parquet\n",
      "    - train_annotation/CalMS21_supplemental/379641005.parquet\n",
      "    - train_annotation/CalMS21_supplemental/397643142.parquet\n",
      "    - train_annotation/CalMS21_supplemental/398952794.parquet\n",
      "    - train_annotation/CalMS21_supplemental/40742782.parquet\n",
      "    - train_annotation/CalMS21_supplemental/40750804.parquet\n",
      "    - train_annotation/CalMS21_supplemental/416793024.parquet\n",
      "    - train_annotation/CalMS21_supplemental/419242179.parquet\n",
      "    - train_annotation/CalMS21_supplemental/423087515.parquet\n",
      "    - train_annotation/CalMS21_supplemental/42319786.parquet\n",
      "    - train_annotation/CalMS21_supplemental/427319973.parquet\n",
      "    - train_annotation/CalMS21_supplemental/427774009.parquet\n",
      "    - train_annotation/CalMS21_supplemental/436212660.parquet\n",
      "    - train_annotation/CalMS21_supplemental/446352765.parquet\n",
      "    - train_annotation/CalMS21_supplemental/452068803.parquet\n",
      "    - train_annotation/CalMS21_supplemental/455337890.parquet\n",
      "    - train_annotation/CalMS21_supplemental/456109830.parquet\n",
      "    - train_annotation/CalMS21_supplemental/461440057.parquet\n",
      "    - train_annotation/CalMS21_supplemental/467701507.parquet\n",
      "    - train_annotation/CalMS21_supplemental/48387399.parquet\n",
      "    - train_annotation/CalMS21_supplemental/495791948.parquet\n",
      "    - train_annotation/CalMS21_supplemental/501836189.parquet\n",
      "    - train_annotation/CalMS21_supplemental/502201747.parquet\n",
      "    - train_annotation/CalMS21_supplemental/506737762.parquet\n",
      "    - train_annotation/CalMS21_supplemental/512573715.parquet\n",
      "    - train_annotation/CalMS21_supplemental/518945549.parquet\n",
      "    - train_annotation/CalMS21_supplemental/518985954.parquet\n",
      "    - train_annotation/CalMS21_supplemental/52039908.parquet\n",
      "    - train_annotation/CalMS21_supplemental/522883468.parquet\n",
      "    - train_annotation/CalMS21_supplemental/531583938.parquet\n",
      "    - train_annotation/CalMS21_supplemental/531669534.parquet\n",
      "    - train_annotation/CalMS21_supplemental/537555916.parquet\n",
      "    - train_annotation/CalMS21_supplemental/54306566.parquet\n",
      "    - train_annotation/CalMS21_supplemental/543628016.parquet\n",
      "    - train_annotation/CalMS21_supplemental/544047008.parquet\n",
      "    - train_annotation/CalMS21_supplemental/550605327.parquet\n",
      "    - train_annotation/CalMS21_supplemental/552808855.parquet\n",
      "    - train_annotation/CalMS21_supplemental/553713693.parquet\n",
      "    - train_annotation/CalMS21_supplemental/560149373.parquet\n",
      "    - train_annotation/CalMS21_supplemental/56352203.parquet\n",
      "    - train_annotation/CalMS21_supplemental/564614304.parquet\n",
      "    - train_annotation/CalMS21_supplemental/566896495.parquet\n",
      "    - train_annotation/CalMS21_supplemental/571702992.parquet\n",
      "    - train_annotation/CalMS21_supplemental/577041568.parquet\n",
      "    - train_annotation/CalMS21_supplemental/583997490.parquet\n",
      "    - train_annotation/CalMS21_supplemental/591324785.parquet\n",
      "    - train_annotation/CalMS21_supplemental/598974879.parquet\n",
      "    - train_annotation/CalMS21_supplemental/605925180.parquet\n",
      "    - train_annotation/CalMS21_supplemental/616524351.parquet\n",
      "    - train_annotation/CalMS21_supplemental/629313314.parquet\n",
      "    - train_annotation/CalMS21_supplemental/64642705.parquet\n",
      "    - train_annotation/CalMS21_supplemental/650064423.parquet\n",
      "... (truncated)\n",
      "... (truncated)\n",
      "... (truncated)\n",
      "\n",
      "[tree] /kaggle/input/mabe-challenge-dataset\n",
      "- models/\n",
      "  - models/lgbm_allogroom.txt\n",
      "  - models/lgbm_approach.txt\n",
      "  - models/lgbm_attack.txt\n",
      "  - models/lgbm_attemptmount.txt\n",
      "  - models/lgbm_avoid.txt\n",
      "  - models/lgbm_biteobject.txt\n",
      "  - models/lgbm_chase.txt\n",
      "  - models/lgbm_chaseattack.txt\n",
      "  - models/lgbm_climb.txt\n",
      "  - models/lgbm_defend.txt\n",
      "  - models/lgbm_dig.txt\n",
      "  - models/lgbm_disengage.txt\n",
      "  - models/lgbm_dominance.txt\n",
      "  - models/lgbm_dominancegroom.txt\n",
      "  - models/lgbm_dominancemount.txt\n",
      "  - models/lgbm_escape.txt\n",
      "  - models/lgbm_exploreobject.txt\n",
      "  - models/lgbm_flinch.txt\n",
      "  - models/lgbm_follow.txt\n",
      "  - models/lgbm_freeze.txt\n",
      "  - models/lgbm_genitalgroom.txt\n",
      "  - models/lgbm_huddle.txt\n",
      "  - models/lgbm_intromit.txt\n",
      "  - models/lgbm_mount.txt\n",
      "  - models/lgbm_rear.txt\n",
      "  - models/lgbm_reciprocalsniff.txt\n",
      "  - models/lgbm_rest.txt\n",
      "  - models/lgbm_run.txt\n",
      "  - models/lgbm_selfgroom.txt\n",
      "  - models/lgbm_shepherd.txt\n",
      "  - models/lgbm_sniff.txt\n",
      "  - models/lgbm_sniffbody.txt\n",
      "  - models/lgbm_sniffface.txt\n",
      "  - models/lgbm_sniffgenital.txt\n",
      "  - models/lgbm_submit.txt\n",
      "  - models/lgbm_tussle.txt\n",
      "- thresholds.joblib\n",
      "\n",
      "OK: required files exist.\n"
     ]
    }
   ],
   "source": [
    "# 2) 检查 Kaggle 输入目录与文件是否齐全（并打印树）\n",
    "\n",
    "def _find_comp_data_dir() -> Path:\n",
    "    base = Path(\"/kaggle/input\")\n",
    "    if not base.exists():\n",
    "        raise RuntimeError(\"This notebook is intended to run on Kaggle. /kaggle/input not found.\")\n",
    "\n",
    "    candidates = []\n",
    "    for p in base.iterdir():\n",
    "        if not p.is_dir():\n",
    "            continue\n",
    "        if (p / \"test.csv\").exists() and (p / \"sample_submission.csv\").exists():\n",
    "            candidates.append(p)\n",
    "\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\"Could not find competition data dir containing test.csv under /kaggle/input\")\n",
    "\n",
    "    # if multiple, prefer one that also has test_tracking\n",
    "    candidates.sort(key=lambda x: (not (x / \"test_tracking\").exists(), x.name))\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "def _tree(path: Path, max_depth: int = 3, max_entries: int = 300) -> None:\n",
    "    path = Path(path)\n",
    "    print(f\"\\n[tree] {path}\")\n",
    "    count = 0\n",
    "\n",
    "    def rec(cur: Path, depth: int) -> None:\n",
    "        nonlocal count\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "        try:\n",
    "            entries = sorted(cur.iterdir(), key=lambda p: (p.is_file(), p.name))\n",
    "        except Exception:\n",
    "            return\n",
    "        for e in entries:\n",
    "            if count >= max_entries:\n",
    "                print(\"... (truncated)\")\n",
    "                return\n",
    "            rel = e.relative_to(path)\n",
    "            print(\"  \" * depth + (\"- \" + str(rel) + (\"/\" if e.is_dir() else \"\")))\n",
    "            count += 1\n",
    "            if e.is_dir():\n",
    "                rec(e, depth + 1)\n",
    "\n",
    "    rec(path, 0)\n",
    "\n",
    "\n",
    "COMP_DATA_DIR = _find_comp_data_dir() if COMP_DATA_DIR is None else Path(COMP_DATA_DIR)\n",
    "print(\"Detected COMP_DATA_DIR:\", COMP_DATA_DIR)\n",
    "\n",
    "ARTIFACT_DIR = Path(\"/kaggle/input\") / ARTIFACT_DATASET_NAME if ARTIFACT_DIR is None else Path(ARTIFACT_DIR)\n",
    "print(\"ARTIFACT_DIR:\", ARTIFACT_DIR)\n",
    "\n",
    "# Show input dirs\n",
    "print(\"\\nAvailable /kaggle/input dirs:\")\n",
    "for p in sorted(Path(\"/kaggle/input\").iterdir()):\n",
    "    if p.is_dir():\n",
    "        print(\"-\", p.name)\n",
    "\n",
    "_tree(COMP_DATA_DIR, max_depth=2)\n",
    "_tree(ARTIFACT_DIR, max_depth=3)\n",
    "\n",
    "# required files\n",
    "REQ = [\n",
    "    COMP_DATA_DIR / \"test.csv\",\n",
    "    COMP_DATA_DIR / \"sample_submission.csv\",\n",
    "    ARTIFACT_DIR / \"thresholds.joblib\",\n",
    "    ARTIFACT_DIR / \"models\",\n",
    "]\n",
    "\n",
    "for r in REQ:\n",
    "    assert r.exists(), f\"Missing required path: {r}\"\n",
    "\n",
    "print(\"\\nOK: required files exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T12:39:48.543723Z",
     "iopub.status.busy": "2025-12-13T12:39:48.543462Z",
     "iopub.status.idle": "2025-12-13T12:39:48.552600Z",
     "shell.execute_reply": "2025-12-13T12:39:48.551219Z",
     "shell.execute_reply.started": "2025-12-13T12:39:48.543704Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions:\n",
      "python 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
      "numpy 1.26.4\n",
      "pandas 2.2.3\n",
      "joblib 1.5.2\n",
      "scipy 1.15.3\n",
      "lightgbm 4.6.0\n"
     ]
    }
   ],
   "source": [
    "# 3) 安装依赖（按需 pip 安装）\n",
    "import importlib\n",
    "\n",
    "def _ensure(pkg: str, pip_name: str | None = None):\n",
    "    try:\n",
    "        return importlib.import_module(pkg)\n",
    "    except Exception:\n",
    "        name = pip_name or pkg\n",
    "        print(f\"Installing {name}...\")\n",
    "        subprocess.check_call([\"python\", \"-m\", \"pip\", \"install\", \"-q\", name])\n",
    "        return importlib.import_module(pkg)\n",
    "\n",
    "# Kaggle 通常自带这些；缺了再装\n",
    "np = _ensure(\"numpy\")\n",
    "pd = _ensure(\"pandas\")\n",
    "joblib = _ensure(\"joblib\")\n",
    "scipy = _ensure(\"scipy\")\n",
    "\n",
    "# LightGBM 推理必需\n",
    "lgb = _ensure(\"lightgbm\", \"lightgbm==4.6.0\")\n",
    "\n",
    "from scipy.signal import medfilt\n",
    "\n",
    "print(\"Versions:\")\n",
    "import sys\n",
    "print(\"python\", sys.version)\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"pandas\", pd.__version__)\n",
    "print(\"joblib\", joblib.__version__)\n",
    "print(\"scipy\", scipy.__version__)\n",
    "print(\"lightgbm\", lgb.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T12:39:48.554014Z",
     "iopub.status.busy": "2025-12-13T12:39:48.553700Z",
     "iopub.status.idle": "2025-12-13T12:39:48.581919Z",
     "shell.execute_reply": "2025-12-13T12:39:48.580870Z",
     "shell.execute_reply.started": "2025-12-13T12:39:48.553979Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No label_map.json (OK for this competition).\n"
     ]
    }
   ],
   "source": [
    "# 4) 加载配置与标签映射（可选）\n",
    "# 本项目的 LightGBM 推理只依赖：models/ + thresholds.joblib。\n",
    "# 这里保留一个“可选配置加载”模板（有则读，没有则跳过）。\n",
    "\n",
    "CONFIG_PATHS = [\n",
    "    ARTIFACT_DIR / \"config.json\",\n",
    "    ARTIFACT_DIR / \"config.yaml\",\n",
    "    ARTIFACT_DIR / \"config.yml\",\n",
    "]\n",
    "\n",
    "config: dict[str, Any] = {}\n",
    "for p in CONFIG_PATHS:\n",
    "    if p.exists() and p.is_file():\n",
    "        if p.suffix == \".json\":\n",
    "            config = json.loads(p.read_text())\n",
    "        else:\n",
    "            # yaml not guaranteed installed; ignore by default\n",
    "            print(f\"Found {p.name} but YAML loader not included; skipping.\")\n",
    "        print(\"Loaded config from:\", p)\n",
    "        break\n",
    "\n",
    "# 可选 label_map\n",
    "LABEL_MAP_PATH = ARTIFACT_DIR / \"label_map.json\"\n",
    "label_map: dict[str, Any] | None = None\n",
    "if LABEL_MAP_PATH.exists():\n",
    "    label_map = json.loads(LABEL_MAP_PATH.read_text())\n",
    "    print(\"Loaded label_map.json\")\n",
    "else:\n",
    "    print(\"No label_map.json (OK for this competition).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T12:39:48.585029Z",
     "iopub.status.busy": "2025-12-13T12:39:48.584720Z",
     "iopub.status.idle": "2025-12-13T12:39:51.218538Z",
     "shell.execute_reply": "2025-12-13T12:39:51.217615Z",
     "shell.execute_reply.started": "2025-12-13T12:39:48.585007Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model expects n_features: 40\n",
      "Example feature cols (head): ['Column_0', 'Column_1', 'Column_2', 'Column_3', 'Column_4', 'Column_5', 'Column_6', 'Column_7', 'Column_8', 'Column_9']\n",
      "Loaded LGBM models: 36\n",
      "Threshold keys: 36\n"
     ]
    }
   ],
   "source": [
    "# 5) 加载模型（PyTorch / TensorFlow / ONNX 三选一模板）\n",
    "# 本项目默认用 LightGBM（MODEL_TYPE='lgbm'）。其它模板仅作参考。\n",
    "\n",
    "\n",
    "def load_lgbm_models(artifact_dir: Path) -> dict[str, lgb.Booster]:\n",
    "    models: dict[str, lgb.Booster] = {}\n",
    "    for p in (artifact_dir / \"models\").glob(\"lgbm_*.txt\"):\n",
    "        action = p.stem.replace(\"lgbm_\", \"\")\n",
    "        models[action] = lgb.Booster(model_file=str(p))\n",
    "    return models\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    if MODEL_TYPE == \"lgbm\":\n",
    "        models = load_lgbm_models(ARTIFACT_DIR)\n",
    "        thresholds = joblib.load(ARTIFACT_DIR / \"thresholds.joblib\")\n",
    "        assert models, \"No lgbm_*.txt found under ARTIFACT_DIR/models\"\n",
    "\n",
    "        # IMPORTANT (feature alignment):\n",
    "        # These models were trained from a numpy matrix without passing explicit feature names.\n",
    "        # LightGBM therefore stores generic names like 'Column_0'..'Column_39'.\n",
    "        # If we follow Booster.feature_name() at inference time, it will NOT match our engineered\n",
    "        # columns (dx/dist/...), and any \"fill missing columns\" fallback would silently feed all-zero\n",
    "        # features -> degenerate predictions (often only one action survives thresholding).\n",
    "        #\n",
    "        # So we only record the expected feature COUNT here; inference will use FEATURE_COLS order.\n",
    "        first = next(iter(models.values()))\n",
    "        n_feat = int(first.num_feature())\n",
    "\n",
    "        # optional sanity check: all models should share identical feature schema\n",
    "        for name, m in models.items():\n",
    "            if int(m.num_feature()) != n_feat:\n",
    "                print(f\"[WARN] action={name} num_feature differs: {int(m.num_feature())} vs {n_feat}\")\n",
    "\n",
    "        print(\"Model expects n_features:\", n_feat)\n",
    "\n",
    "        return {\"models\": models, \"thresholds\": thresholds, \"n_features\": n_feat}\n",
    "\n",
    "    if MODEL_TYPE == \"pytorch\":\n",
    "        import torch\n",
    "        # TODO: 你需要自己实现/导入网络结构，并加载权重\n",
    "        # model = YourModel(...)\n",
    "        # state = torch.load(MODEL_PATH, map_location='cpu')\n",
    "        # model.load_state_dict(state)\n",
    "        # model.eval()\n",
    "        raise NotImplementedError(\"PyTorch template: implement your own model loading.\")\n",
    "\n",
    "    if MODEL_TYPE == \"tensorflow\":\n",
    "        import tensorflow as tf\n",
    "        # model = tf.keras.models.load_model(MODEL_PATH)\n",
    "        raise NotImplementedError(\"TensorFlow template: implement your own model loading.\")\n",
    "\n",
    "    if MODEL_TYPE == \"onnx\":\n",
    "        import onnxruntime as ort\n",
    "        # sess = ort.InferenceSession(MODEL_PATH, providers=[...])\n",
    "        raise NotImplementedError(\"ONNX template: implement your own model loading.\")\n",
    "\n",
    "    raise ValueError(MODEL_TYPE)\n",
    "\n",
    "\n",
    "bundle = build_model()\n",
    "if MODEL_TYPE == \"lgbm\":\n",
    "    print(\"Loaded LGBM models:\", len(bundle[\"models\"]))\n",
    "    print(\"Threshold keys:\", len(bundle[\"thresholds\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T12:39:51.220130Z",
     "iopub.status.busy": "2025-12-13T12:39:51.219837Z",
     "iopub.status.idle": "2025-12-13T12:39:51.239258Z",
     "shell.execute_reply": "2025-12-13T12:39:51.238060Z",
     "shell.execute_reply.started": "2025-12-13T12:39:51.220109Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_DF: (1, 39)\n",
      "           lab_id   video_id\n",
      "0  AdaptableSnail  438887472\n"
     ]
    }
   ],
   "source": [
    "# 6) 加载测试数据（CSV / Parquet / 图片目录 三选一模板）\n",
    "# 本竞赛：读取 test.csv + test_tracking/*.parquet\n",
    "\n",
    "\n",
    "def _json_load_maybe(value: Any) -> Any:\n",
    "    if value is None:\n",
    "        return None\n",
    "    if isinstance(value, float) and np.isnan(value):\n",
    "        return None\n",
    "    if isinstance(value, (list, dict)):\n",
    "        return value\n",
    "    s = str(value).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_behaviors_labeled(s: Any) -> list[tuple[int, object, str]]:\n",
    "    \"\"\"Parse behaviors_labeled into (agent_raw, target_raw, action).\n",
    "\n",
    "    Raw format: \"mouse1,mouse2,approach\" or \"mouse1,self,rear\".\n",
    "    Keep target_raw='self' literal so submission can match public notebooks / official metric expectations.\n",
    "    \"\"\"\n",
    "    v = _json_load_maybe(s)\n",
    "    if v is None:\n",
    "        return []\n",
    "\n",
    "    out: list[tuple[int, object, str]] = []\n",
    "    for item in v:\n",
    "        parts = str(item).split(\",\")\n",
    "        if len(parts) != 3:\n",
    "            continue\n",
    "        a_raw, t_raw, action = [p.strip() for p in parts]\n",
    "        if not a_raw.startswith(\"mouse\"):\n",
    "            continue\n",
    "        try:\n",
    "            agent = int(a_raw.replace(\"mouse\", \"\"))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if t_raw == \"self\":\n",
    "            target: object = \"self\"\n",
    "        else:\n",
    "            if not t_raw.startswith(\"mouse\"):\n",
    "                continue\n",
    "            try:\n",
    "                target = int(t_raw.replace(\"mouse\", \"\"))\n",
    "            except Exception:\n",
    "                continue\n",
    "        out.append((agent, target, str(action)))\n",
    "    return out\n",
    "\n",
    "\n",
    "TEST_DF = pd.read_csv(COMP_DATA_DIR / \"test.csv\")\n",
    "TEST_DF[\"behaviors_labeled_list\"] = TEST_DF[\"behaviors_labeled\"].apply(parse_behaviors_labeled)\n",
    "\n",
    "assert \"video_id\" in TEST_DF.columns\n",
    "assert \"lab_id\" in TEST_DF.columns\n",
    "print(\"TEST_DF:\", TEST_DF.shape)\n",
    "print(TEST_DF[[\"lab_id\", \"video_id\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T12:39:51.240553Z",
     "iopub.status.busy": "2025-12-13T12:39:51.240331Z",
     "iopub.status.idle": "2025-12-13T12:39:51.282616Z",
     "shell.execute_reply": "2025-12-13T12:39:51.281471Z",
     "shell.execute_reply.started": "2025-12-13T12:39:51.240536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 7) 数据预处理与 Batching（按任务改）\n",
    "# 本项目推理只有 1 个测试视频，也可以直接全量按帧算特征（不必复杂 batching）。\n",
    "\n",
    "CENTER_PART_PRIORITY = [\"body_center\", \"neck\"]\n",
    "NOSE_PART_PRIORITY = [\"nose\"]\n",
    "TAIL_PART_PRIORITY = [\"tail_base\", \"tail_midpoint\"]\n",
    "\n",
    "\n",
    "def _pick_available_part(available: set[str], priority: list[str]) -> str | None:\n",
    "    for p in priority:\n",
    "        if p in available:\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "\n",
    "def _get_tracking_path(split_dir: Path, lab_id: str, video_id: int) -> Path:\n",
    "    # Kaggle data provides train_tracking/ and test_tracking/\n",
    "    return split_dir / str(lab_id) / f\"{int(video_id)}.parquet\"\n",
    "\n",
    "\n",
    "def load_tracking_df(split: str, lab_id: str, video_id: int) -> pd.DataFrame:\n",
    "    split_dir = COMP_DATA_DIR / (\"test_tracking\" if split == \"test\" else \"train_tracking\")\n",
    "    path = _get_tracking_path(split_dir, lab_id, video_id)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    df = pd.read_parquet(path)\n",
    "    need = {\"video_frame\", \"mouse_id\", \"bodypart\", \"x\", \"y\"}\n",
    "    missing = need - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Tracking parquet missing columns: {missing}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_mouse_state(tracking_df: pd.DataFrame, fps: float, pix_per_cm: float | None, interp_limit: int = 5) -> pd.DataFrame:\n",
    "    df = tracking_df.copy()\n",
    "    if df[\"mouse_id\"].dtype == object:\n",
    "        df[\"mouse\"] = df[\"mouse_id\"].astype(str).str.replace(\"mouse\", \"\", regex=False).astype(int)\n",
    "    else:\n",
    "        df[\"mouse\"] = df[\"mouse_id\"].astype(int)\n",
    "    df[\"video_frame\"] = df[\"video_frame\"].astype(int)\n",
    "\n",
    "    available_parts = set(df[\"bodypart\"].astype(str).unique())\n",
    "    center_part = _pick_available_part(available_parts, CENTER_PART_PRIORITY)\n",
    "    nose_part = _pick_available_part(available_parts, NOSE_PART_PRIORITY)\n",
    "    tail_part = _pick_available_part(available_parts, TAIL_PART_PRIORITY)\n",
    "\n",
    "    mice = sorted(df[\"mouse\"].unique().astype(int).tolist())\n",
    "    max_frame = int(df[\"video_frame\"].max())\n",
    "    full_index = pd.MultiIndex.from_product(\n",
    "        [np.arange(0, max_frame + 1, dtype=np.int32), mice],\n",
    "        names=[\"video_frame\", \"mouse\"],\n",
    "    )\n",
    "\n",
    "    def _extract_part(part: str) -> pd.DataFrame:\n",
    "        part_df = df.loc[df[\"bodypart\"].astype(str) == part, [\"video_frame\", \"mouse\", \"x\", \"y\"]].copy()\n",
    "        part_df = part_df.rename(columns={\"x\": f\"{part}_x\", \"y\": f\"{part}_y\"})\n",
    "        part_df = part_df.set_index([\"video_frame\", \"mouse\"]).sort_index()\n",
    "        return part_df\n",
    "\n",
    "    def _to_cm(part_df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "        if pix_per_cm and np.isfinite(pix_per_cm) and pix_per_cm > 0:\n",
    "            part_df[cols] = part_df[cols] / float(pix_per_cm)\n",
    "        return part_df\n",
    "\n",
    "    # center\n",
    "    if center_part is not None:\n",
    "        center_xy = _extract_part(center_part)\n",
    "        center_xy = center_xy.rename(columns={f\"{center_part}_x\": \"center_x\", f\"{center_part}_y\": \"center_y\"}).reindex(full_index)\n",
    "        center_xy = _to_cm(center_xy, [\"center_x\", \"center_y\"])\n",
    "        out = center_xy\n",
    "    else:\n",
    "        if {\"nose\", \"tail_base\"}.issubset(available_parts):\n",
    "            parts_for_center = [\"nose\", \"tail_base\"]\n",
    "        elif {\"ear_left\", \"ear_right\"}.issubset(available_parts):\n",
    "            parts_for_center = [\"ear_left\", \"ear_right\"]\n",
    "        else:\n",
    "            parts_for_center = [p for p in [\"nose\", \"tail_base\", \"ear_left\", \"ear_right\", \"tail_tip\"] if p in available_parts]\n",
    "            if not parts_for_center:\n",
    "                parts_for_center = sorted(list(available_parts))[:2]\n",
    "\n",
    "        part_dfs = []\n",
    "        for p in parts_for_center:\n",
    "            d = _extract_part(p).reindex(full_index)\n",
    "            d = d.rename(columns={f\"{p}_x\": f\"x_{p}\", f\"{p}_y\": f\"y_{p}\"})\n",
    "            d = _to_cm(d, [f\"x_{p}\", f\"y_{p}\"])\n",
    "            part_dfs.append(d)\n",
    "\n",
    "        merged = pd.concat(part_dfs, axis=1)\n",
    "        x_cols = [c for c in merged.columns if c.startswith(\"x_\")]\n",
    "        y_cols = [c for c in merged.columns if c.startswith(\"y_\")]\n",
    "        out = pd.DataFrame(index=full_index)\n",
    "        out[\"center_x\"] = merged[x_cols].mean(axis=1, skipna=True)\n",
    "        out[\"center_y\"] = merged[y_cols].mean(axis=1, skipna=True)\n",
    "\n",
    "    out[[\"center_x\", \"center_y\"]] = (\n",
    "        out.groupby(level=\"mouse\", group_keys=False)[[\"center_x\", \"center_y\"]]\n",
    "        .apply(lambda g: g.interpolate(limit=interp_limit, limit_direction=\"both\"))\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "    out[[\"vx\", \"vy\"]] = out.groupby(level=\"mouse\", group_keys=False)[[\"center_x\", \"center_y\"]].diff().fillna(0.0) * float(fps)\n",
    "    out[\"speed\"] = np.sqrt(out[\"vx\"] ** 2 + out[\"vy\"] ** 2)\n",
    "\n",
    "    # heading from nose\n",
    "    out[\"hx\"] = 0.0\n",
    "    out[\"hy\"] = 0.0\n",
    "    out[\"has_heading\"] = np.int8(0)\n",
    "\n",
    "    nose_xy = None\n",
    "    if nose_part is not None:\n",
    "        nose_xy = _extract_part(nose_part).rename(columns={f\"{nose_part}_x\": \"nose_x\", f\"{nose_part}_y\": \"nose_y\"}).reindex(full_index)\n",
    "        nose_xy = _to_cm(nose_xy, [\"nose_x\", \"nose_y\"])\n",
    "        nose_xy[[\"nose_x\", \"nose_y\"]] = (\n",
    "            nose_xy.groupby(level=\"mouse\", group_keys=False)[[\"nose_x\", \"nose_y\"]]\n",
    "            .apply(lambda g: g.interpolate(limit=interp_limit, limit_direction=\"both\"))\n",
    "            .fillna(0.0)\n",
    "        )\n",
    "\n",
    "        dx = nose_xy[\"nose_x\"] - out[\"center_x\"]\n",
    "        dy = nose_xy[\"nose_y\"] - out[\"center_y\"]\n",
    "        norm = np.sqrt(dx * dx + dy * dy)\n",
    "        ok = norm > 1e-6\n",
    "        out.loc[ok, \"hx\"] = (dx.loc[ok] / norm.loc[ok]).astype(float)\n",
    "        out.loc[ok, \"hy\"] = (dy.loc[ok] / norm.loc[ok]).astype(float)\n",
    "        out.loc[ok, \"has_heading\"] = np.int8(1)\n",
    "\n",
    "    # body len\n",
    "    out[\"body_len\"] = 0.0\n",
    "    if tail_part is not None and nose_xy is not None:\n",
    "        tail_xy = _extract_part(tail_part).rename(columns={f\"{tail_part}_x\": \"tail_x\", f\"{tail_part}_y\": \"tail_y\"}).reindex(full_index)\n",
    "        tail_xy = _to_cm(tail_xy, [\"tail_x\", \"tail_y\"])\n",
    "        tail_xy[[\"tail_x\", \"tail_y\"]] = (\n",
    "            tail_xy.groupby(level=\"mouse\", group_keys=False)[[\"tail_x\", \"tail_y\"]]\n",
    "            .apply(lambda g: g.interpolate(limit=interp_limit, limit_direction=\"both\"))\n",
    "            .fillna(0.0)\n",
    "        )\n",
    "        out[\"body_len\"] = np.sqrt((nose_xy[\"nose_x\"] - tail_xy[\"tail_x\"]) ** 2 + (nose_xy[\"nose_y\"] - tail_xy[\"tail_y\"]) ** 2).astype(float)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _rolling_feats(s: pd.Series, windows: tuple[int, ...]) -> dict[str, pd.Series]:\n",
    "    out: dict[str, pd.Series] = {}\n",
    "    for w in windows:\n",
    "        r = s.rolling(window=w, min_periods=1, center=True)\n",
    "        out[f\"mean_{w}\"] = r.mean()\n",
    "        out[f\"std_{w}\"] = r.std(ddof=0).fillna(0.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def pair_features_for_frames(state: pd.DataFrame, agent: int, target: int, frames: np.ndarray) -> pd.DataFrame:\n",
    "    # Robustness: if behaviors_labeled references a mouse id not present in tracking for this video,\n",
    "    # return all-zero features for those frames (so the model predicts background).\n",
    "    available_mice = set(state.index.get_level_values(\"mouse\").unique().astype(int).tolist())\n",
    "    if int(agent) not in available_mice or int(target) not in available_mice:\n",
    "        out = pd.DataFrame(0.0, index=frames, columns=FEATURE_COLS)\n",
    "        for c in [\"agent_has_heading\", \"target_has_heading\"]:\n",
    "            if c in out.columns:\n",
    "                out[c] = out[c].astype(np.int8)\n",
    "        return out\n",
    "\n",
    "    a = state.xs(agent, level=\"mouse\").reindex(frames)\n",
    "    t = state.xs(target, level=\"mouse\").reindex(frames)\n",
    "\n",
    "    dx = (t[\"center_x\"] - a[\"center_x\"]).astype(float)\n",
    "    dy = (t[\"center_y\"] - a[\"center_y\"]).astype(float)\n",
    "    dist = np.sqrt(dx * dx + dy * dy) + 1e-6\n",
    "\n",
    "    rel_vx = (t[\"vx\"] - a[\"vx\"]).astype(float)\n",
    "    rel_vy = (t[\"vy\"] - a[\"vy\"]).astype(float)\n",
    "    closing = (dx * rel_vx + dy * rel_vy) / dist\n",
    "    cos_facing = (a[\"hx\"] * dx + a[\"hy\"] * dy) / dist\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"dx\": dx,\n",
    "            \"dy\": dy,\n",
    "            \"dist\": dist,\n",
    "            \"agent_speed\": a[\"speed\"].astype(float),\n",
    "            \"target_speed\": t[\"speed\"].astype(float),\n",
    "            \"closing\": closing.astype(float),\n",
    "            \"cos_facing\": cos_facing.astype(float),\n",
    "            \"agent_has_heading\": a[\"has_heading\"].fillna(0).astype(np.int8),\n",
    "            \"target_has_heading\": t[\"has_heading\"].fillna(0).astype(np.int8),\n",
    "            \"agent_body_len\": a[\"body_len\"].fillna(0.0).astype(float),\n",
    "            \"target_body_len\": t[\"body_len\"].fillna(0.0).astype(float),\n",
    "        },\n",
    "        index=frames,\n",
    "    )\n",
    "\n",
    "    out[\"dist_diff1\"] = out[\"dist\"].diff().fillna(0.0)\n",
    "    out[\"closing_diff1\"] = out[\"closing\"].diff().fillna(0.0)\n",
    "    out[\"cos_facing_diff1\"] = out[\"cos_facing\"].diff().fillna(0.0)\n",
    "    out[\"agent_speed_diff1\"] = out[\"agent_speed\"].diff().fillna(0.0)\n",
    "    out[\"target_speed_diff1\"] = out[\"target_speed\"].diff().fillna(0.0)\n",
    "\n",
    "    windows = (5, 15)\n",
    "    for k, s in _rolling_feats(out[\"dist\"], windows).items():\n",
    "        out[f\"dist_{k}\"] = s\n",
    "    for k, s in _rolling_feats(out[\"closing\"], windows).items():\n",
    "        out[f\"closing_{k}\"] = s\n",
    "    for k, s in _rolling_feats(out[\"cos_facing\"], windows).items():\n",
    "        out[f\"cos_facing_{k}\"] = s\n",
    "    for k, s in _rolling_feats(out[\"agent_speed\"], windows).items():\n",
    "        out[f\"agent_speed_{k}\"] = s\n",
    "    for k, s in _rolling_feats(out[\"target_speed\"], windows).items():\n",
    "        out[f\"target_speed_{k}\"] = s\n",
    "\n",
    "    # Relative scale features (these are the extra +4 features that make 40 total)\n",
    "    eps = 1e-6\n",
    "    out[\"dist_rel_agent\"] = out[\"dist\"] / (out[\"agent_body_len\"] + eps)\n",
    "    out[\"dist_rel_target\"] = out[\"dist\"] / (out[\"target_body_len\"] + eps)\n",
    "    out[\"agent_speed_rel\"] = out[\"agent_speed\"] / (out[\"agent_body_len\"] + eps)\n",
    "    out[\"target_speed_rel\"] = out[\"target_speed\"] / (out[\"target_body_len\"] + eps)\n",
    "\n",
    "    out = out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"dx\",\n",
    "    \"dy\",\n",
    "    \"dist\",\n",
    "    \"agent_speed\",\n",
    "    \"target_speed\",\n",
    "    \"closing\",\n",
    "    \"cos_facing\",\n",
    "    \"agent_has_heading\",\n",
    "    \"target_has_heading\",\n",
    "    \"agent_body_len\",\n",
    "    \"target_body_len\",\n",
    "    \"dist_diff1\",\n",
    "    \"closing_diff1\",\n",
    "    \"cos_facing_diff1\",\n",
    "    \"agent_speed_diff1\",\n",
    "    \"target_speed_diff1\",\n",
    "    \"dist_mean_5\",\n",
    "    \"dist_std_5\",\n",
    "    \"dist_mean_15\",\n",
    "    \"dist_std_15\",\n",
    "    \"closing_mean_5\",\n",
    "    \"closing_std_5\",\n",
    "    \"closing_mean_15\",\n",
    "    \"closing_std_15\",\n",
    "    \"cos_facing_mean_5\",\n",
    "    \"cos_facing_std_5\",\n",
    "    \"cos_facing_mean_15\",\n",
    "    \"cos_facing_std_15\",\n",
    "    \"agent_speed_mean_5\",\n",
    "    \"agent_speed_std_5\",\n",
    "    \"agent_speed_mean_15\",\n",
    "    \"agent_speed_std_15\",\n",
    "    \"target_speed_mean_5\",\n",
    "    \"target_speed_std_5\",\n",
    "    \"target_speed_mean_15\",\n",
    "    \"target_speed_std_15\",\n",
    "    \"dist_rel_agent\",\n",
    "    \"dist_rel_target\",\n",
    "    \"agent_speed_rel\",\n",
    "    \"target_speed_rel\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T12:39:51.284125Z",
     "iopub.status.busy": "2025-12-13T12:39:51.283826Z",
     "iopub.status.idle": "2025-12-13T12:41:17.691961Z",
     "shell.execute_reply": "2025-12-13T12:41:17.690987Z",
     "shell.execute_reply.started": "2025-12-13T12:39:51.284095Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using feature_cols from: bundle\n",
      "n_features: 40\n",
      "submission rows: 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>target_id</th>\n",
       "      <th>action</th>\n",
       "      <th>start_frame</th>\n",
       "      <th>stop_frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse1</td>\n",
       "      <td>mouse2</td>\n",
       "      <td>chaseattack</td>\n",
       "      <td>0</td>\n",
       "      <td>18423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse1</td>\n",
       "      <td>mouse3</td>\n",
       "      <td>chaseattack</td>\n",
       "      <td>0</td>\n",
       "      <td>18423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse1</td>\n",
       "      <td>mouse4</td>\n",
       "      <td>chaseattack</td>\n",
       "      <td>0</td>\n",
       "      <td>18423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse2</td>\n",
       "      <td>mouse1</td>\n",
       "      <td>chaseattack</td>\n",
       "      <td>0</td>\n",
       "      <td>18423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse2</td>\n",
       "      <td>mouse3</td>\n",
       "      <td>chaseattack</td>\n",
       "      <td>0</td>\n",
       "      <td>18423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse2</td>\n",
       "      <td>mouse4</td>\n",
       "      <td>chaseattack</td>\n",
       "      <td>0</td>\n",
       "      <td>18423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse3</td>\n",
       "      <td>mouse1</td>\n",
       "      <td>chaseattack</td>\n",
       "      <td>0</td>\n",
       "      <td>18423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse3</td>\n",
       "      <td>mouse2</td>\n",
       "      <td>chaseattack</td>\n",
       "      <td>0</td>\n",
       "      <td>18423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse3</td>\n",
       "      <td>mouse4</td>\n",
       "      <td>chaseattack</td>\n",
       "      <td>0</td>\n",
       "      <td>18423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse4</td>\n",
       "      <td>mouse1</td>\n",
       "      <td>chaseattack</td>\n",
       "      <td>0</td>\n",
       "      <td>18423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id   video_id agent_id target_id       action  start_frame  stop_frame\n",
       "0       0  438887472   mouse1    mouse2  chaseattack            0       18423\n",
       "1       1  438887472   mouse1    mouse3  chaseattack            0       18423\n",
       "2       2  438887472   mouse1    mouse4  chaseattack            0       18423\n",
       "3       3  438887472   mouse2    mouse1  chaseattack            0       18423\n",
       "4       4  438887472   mouse2    mouse3  chaseattack            0       18423\n",
       "5       5  438887472   mouse2    mouse4  chaseattack            0       18423\n",
       "6       6  438887472   mouse3    mouse1  chaseattack            0       18423\n",
       "7       7  438887472   mouse3    mouse2  chaseattack            0       18423\n",
       "8       8  438887472   mouse3    mouse4  chaseattack            0       18423\n",
       "9       9  438887472   mouse4    mouse1  chaseattack            0       18423"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8) 推理与后处理（对齐官方 metric 约束 + self 输出）\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "\n",
    "def _segments_from_binary(binary: np.ndarray) -> list[tuple[int, int]]:\n",
    "    \"\"\"Return inclusive (start, stop) segments from a 0/1 array.\"\"\"\n",
    "    if binary.size == 0:\n",
    "        return []\n",
    "    binary = binary.astype(np.int8)\n",
    "    changes = np.diff(binary, prepend=0, append=0)\n",
    "    starts = np.where(changes == 1)[0]\n",
    "    ends = np.where(changes == -1)[0] - 1\n",
    "    return list(zip(starts.tolist(), ends.tolist()))\n",
    "\n",
    "\n",
    "def _merge_close_segments(segs: list[tuple[int, int]], gap_frames: int) -> list[tuple[int, int]]:\n",
    "    if not segs:\n",
    "        return []\n",
    "    if gap_frames <= 0:\n",
    "        return segs\n",
    "    segs = sorted(segs)\n",
    "    merged = [segs[0]]\n",
    "    for s, e in segs[1:]:\n",
    "        ps, pe = merged[-1]\n",
    "        if s <= pe + gap_frames + 1:\n",
    "            merged[-1] = (ps, max(pe, e))\n",
    "        else:\n",
    "            merged.append((s, e))\n",
    "    return merged\n",
    "\n",
    "\n",
    "def _smooth_prob(prob: np.ndarray, median_kernel: int) -> np.ndarray:\n",
    "    k = int(median_kernel)\n",
    "    if k % 2 == 0:\n",
    "        k += 1\n",
    "    k = max(3, k)\n",
    "    p = prob.astype(np.float32)\n",
    "    if p.size >= k:\n",
    "        p = medfilt(p, kernel_size=k)\n",
    "    return p\n",
    "\n",
    "\n",
    "def _postprocess_binary_to_segments(binary: np.ndarray, fps: float) -> list[tuple[int, int]]:\n",
    "    segs = _segments_from_binary(binary)\n",
    "    min_len = int(round(float(fps) * float(MIN_DURATION_SEC)))\n",
    "    min_len = max(1, min_len)\n",
    "    segs = [(s, e) for (s, e) in segs if (e - s + 1) >= min_len]\n",
    "    gap_frames = int(round(float(fps) * float(MERGE_GAP_SEC)))\n",
    "    segs = _merge_close_segments(segs, gap_frames=gap_frames)\n",
    "    segs = [(s, e) for (s, e) in segs if (e - s + 1) >= min_len]\n",
    "    return segs\n",
    "\n",
    "\n",
    "def _mouse_num(x: object) -> int | None:\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, (int, np.integer)):\n",
    "        return int(x)\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    if s.startswith(\"mouse\"):\n",
    "        s = s.replace(\"mouse\", \"\")\n",
    "    try:\n",
    "        return int(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _normalize_pair(agent_raw: object, target_raw: object) -> tuple[int, int, str, str] | None:\n",
    "    \"\"\"Return (agent_int, target_int_for_features, agent_id_str, target_id_str_for_submission).\"\"\"\n",
    "    a = _mouse_num(agent_raw)\n",
    "    if a is None or a <= 0:\n",
    "        return None\n",
    "    agent_id_str = f\"mouse{a}\"\n",
    "\n",
    "    # Keep literal self when present (preferred).\n",
    "    if isinstance(target_raw, str) and target_raw.strip() == \"self\":\n",
    "        target_int = a\n",
    "        target_id_str = \"self\" if OUTPUT_SELF_AS_LITERAL else f\"mouse{a}\"\n",
    "        return int(a), int(target_int), agent_id_str, target_id_str\n",
    "\n",
    "    # Robustness: if target == agent (e.g., rear sometimes encoded as mouseX,mouseX),\n",
    "    # output target_id as literal 'self' to match public notebook submissions.\n",
    "    t = _mouse_num(target_raw)\n",
    "    if t is None or t <= 0:\n",
    "        return None\n",
    "    if int(t) == int(a):\n",
    "        target_int = a\n",
    "        target_id_str = \"self\" if OUTPUT_SELF_AS_LITERAL else f\"mouse{a}\"\n",
    "        return int(a), int(target_int), agent_id_str, target_id_str\n",
    "\n",
    "    return int(a), int(t), agent_id_str, f\"mouse{t}\"\n",
    "\n",
    "\n",
    "def run_inference_lgbm(test_df: pd.DataFrame, bundle: dict[str, Any]) -> pd.DataFrame:\n",
    "    models: dict[str, lgb.Booster] = bundle[\"models\"]\n",
    "    thresholds: dict[str, float] = bundle[\"thresholds\"]\n",
    "\n",
    "    # Align inference features to training feature order\n",
    "    # NOTE: do NOT use Booster.feature_name() here (likely 'Column_0'..), see build_model().\n",
    "    n_feat_model = int(bundle.get(\"n_features\", next(iter(models.values())).num_feature()))\n",
    "    feature_cols: list[str] = list(FEATURE_COLS)\n",
    "    if len(feature_cols) != n_feat_model:\n",
    "        raise RuntimeError(\n",
    "            f\"Feature count mismatch: model expects {n_feat_model} but FEATURE_COLS has {len(feature_cols)}\"\n",
    "        )\n",
    "    print(\"Using feature_cols from: FEATURE_COLS\")\n",
    "    print(\"n_features:\", len(feature_cols))\n",
    "\n",
    "    rows: list[list[Any]] = []\n",
    "\n",
    "    for _, meta in test_df.iterrows():\n",
    "        lab_id = str(meta[\"lab_id\"])\n",
    "        video_id = int(meta[\"video_id\"])\n",
    "        fps = float(meta.get(\"frames_per_second\", 30.0) or 30.0)\n",
    "        pix_per_cm = meta.get(\"pix_per_cm_approx\", None)\n",
    "        if pix_per_cm is not None and not np.isfinite(pix_per_cm):\n",
    "            pix_per_cm = None\n",
    "\n",
    "        tracking = load_tracking_df(\"test\", lab_id, video_id)\n",
    "        state = build_mouse_state(tracking, fps=fps, pix_per_cm=pix_per_cm, interp_limit=5)\n",
    "        max_frame = int(state.index.get_level_values(\"video_frame\").max())\n",
    "        frames = np.arange(0, max_frame + 1, dtype=np.int32)\n",
    "\n",
    "        candidates: list[tuple[int, object, str]] = meta[\"behaviors_labeled_list\"]\n",
    "\n",
    "        # group by normalized pair; enforce per-pair per-frame single action (official metric constraint)\n",
    "        pair_to_actions: dict[tuple[int, int, str, str], list[str]] = defaultdict(list)\n",
    "        for agent_raw, target_raw, action in candidates:\n",
    "            if action not in models:\n",
    "                continue\n",
    "            norm = _normalize_pair(agent_raw, target_raw)\n",
    "            if norm is None:\n",
    "                continue\n",
    "            agent_i, target_i, agent_id_str, target_id_str = norm\n",
    "            pair_to_actions[(agent_i, target_i, agent_id_str, target_id_str)].append(str(action))\n",
    "\n",
    "        row_id = 0\n",
    "        for (agent_i, target_i, agent_id_str, target_id_str), actions_for_pair in pair_to_actions.items():\n",
    "            actions_unique: list[str] = list(dict.fromkeys(actions_for_pair))\n",
    "            feats = pair_features_for_frames(state, agent=agent_i, target=target_i, frames=frames)\n",
    "\n",
    "            # Defensive: engineered features must all exist. If this triggers, the notebook is out of sync.\n",
    "            missing = [c for c in feature_cols if c not in feats.columns]\n",
    "            if missing:\n",
    "                raise RuntimeError(f\"Missing engineered features in feats: {missing[:10]} (total={len(missing)})\")\n",
    "\n",
    "            X = feats[feature_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "            probs = []\n",
    "            ths = []\n",
    "            for action in actions_unique:\n",
    "                p = models[action].predict(X)\n",
    "                p = _smooth_prob(p, median_kernel=MEDIAN_KERNEL)\n",
    "                probs.append(p)\n",
    "                ths.append(float(thresholds.get(action, 0.5)))\n",
    "\n",
    "            P = np.vstack(probs)  # (A, T)\n",
    "            masked = P.copy()\n",
    "            for i in range(masked.shape[0]):\n",
    "                masked[i, masked[i] < ths[i]] = -np.inf\n",
    "\n",
    "            winner_idx = masked.argmax(axis=0)\n",
    "            winner_val = masked[winner_idx, np.arange(masked.shape[1])]\n",
    "            active = np.isfinite(winner_val)\n",
    "\n",
    "            for i, action in enumerate(actions_unique):\n",
    "                binary = (active & (winner_idx == i)).astype(np.int8)\n",
    "                segs = _postprocess_binary_to_segments(binary=binary, fps=fps)\n",
    "                for s, e in segs:\n",
    "                    rows.append(\n",
    "                        [\n",
    "                            row_id,\n",
    "                            video_id,\n",
    "                            agent_id_str,\n",
    "                            target_id_str,\n",
    "                            action,\n",
    "                            int(s),\n",
    "                            int(e + 1),  # stop_frame EXCLUSIVE\n",
    "                        ]\n",
    "                    )\n",
    "                    row_id += 1\n",
    "\n",
    "    sub = pd.DataFrame(rows, columns=SUB_COLS)\n",
    "\n",
    "    # Canonical deterministic ordering\n",
    "    def _agent_num(s: object) -> int:\n",
    "        m = re.search(r\"(\\d+)$\", str(s))\n",
    "        return int(m.group(1)) if m else -1\n",
    "\n",
    "    def _target_num(row: pd.Series) -> int:\n",
    "        if str(row[\"target_id\"]) == \"self\":\n",
    "            return int(row[\"agent_num\"])\n",
    "        m = re.search(r\"(\\d+)$\", str(row[\"target_id\"]))\n",
    "        return int(m.group(1)) if m else -1\n",
    "\n",
    "    if len(sub):\n",
    "        sub[\"agent_num\"] = sub[\"agent_id\"].map(_agent_num)\n",
    "        sub[\"target_num\"] = sub.apply(_target_num, axis=1)\n",
    "        sub = (\n",
    "            sub.sort_values([\"video_id\", \"agent_num\", \"target_num\", \"action\", \"start_frame\", \"stop_frame\"], kind=\"mergesort\")\n",
    "            .drop(columns=[\"agent_num\", \"target_num\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    sub[\"row_id\"] = np.arange(len(sub), dtype=np.int64)\n",
    "    return sub\n",
    "\n",
    "\n",
    "assert MODEL_TYPE == \"lgbm\", \"This notebook is pre-wired for LightGBM artifacts.\"\n",
    "submission = run_inference_lgbm(TEST_DF, bundle)\n",
    "print(\"submission rows:\", len(submission))\n",
    "submission.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T12:41:17.693323Z",
     "iopub.status.busy": "2025-12-13T12:41:17.693088Z",
     "iopub.status.idle": "2025-12-13T12:41:17.698570Z",
     "shell.execute_reply": "2025-12-13T12:41:17.697408Z",
     "shell.execute_reply.started": "2025-12-13T12:41:17.693306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 9) （可选）本地验证集评估与简单可视化\n",
    "# Kaggle 测试集没有标签；默认关闭。\n",
    "RUN_EVAL = False\n",
    "\n",
    "if RUN_EVAL:\n",
    "    print(\"No evaluation implemented (test set has no labels).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T12:41:17.700069Z",
     "iopub.status.busy": "2025-12-13T12:41:17.699259Z",
     "iopub.status.idle": "2025-12-13T12:41:17.728878Z",
     "shell.execute_reply": "2025-12-13T12:41:17.728000Z",
     "shell.execute_reply.started": "2025-12-13T12:41:17.700036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /kaggle/working/submission.csv\n",
      "size (bytes): 619\n",
      "has self target? False\n",
      "   row_id   video_id agent_id target_id       action  start_frame  stop_frame\n",
      "0       0  438887472   mouse1    mouse2  chaseattack            0       18423\n",
      "1       1  438887472   mouse1    mouse3  chaseattack            0       18423\n",
      "2       2  438887472   mouse1    mouse4  chaseattack            0       18423\n",
      "3       3  438887472   mouse2    mouse1  chaseattack            0       18423\n",
      "4       4  438887472   mouse2    mouse3  chaseattack            0       18423\n"
     ]
    }
   ],
   "source": [
    "# 10) 生成 submission.csv 并保存到 /kaggle/working\n",
    "\n",
    "# Safety: ensure rear is output as target_id='self' (matches public notebooks).\n",
    "if len(submission):\n",
    "    m_same = (submission[\"action\"].astype(str) == \"rear\") & (submission[\"target_id\"].astype(str) == submission[\"agent_id\"].astype(str))\n",
    "    if bool(m_same.any()):\n",
    "        submission.loc[m_same, \"target_id\"] = \"self\"\n",
    "    m_bad = (submission[\"action\"].astype(str) == \"rear\") & (submission[\"target_id\"].astype(str) != \"self\")\n",
    "    if bool(m_bad.any()):\n",
    "        print(\"[WARN] rear rows have non-self target_id (showing head):\")\n",
    "        print(submission.loc[m_bad].head().to_string(index=False))\n",
    "\n",
    "out_path = WORK_DIR / \"submission.csv\"\n",
    "submission.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"Wrote:\", out_path)\n",
    "print(\"size (bytes):\", out_path.stat().st_size)\n",
    "print(\"has self target?\", bool((submission[\"target_id\"] == \"self\").any()))\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T12:41:17.730342Z",
     "iopub.status.busy": "2025-12-13T12:41:17.729900Z",
     "iopub.status.idle": "2025-12-13T12:41:17.905371Z",
     "shell.execute_reply": "2025-12-13T12:41:17.904453Z",
     "shell.execute_reply.started": "2025-12-13T12:41:17.730318Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /kaggle/working/run_meta.json\n",
      "Wrote: /kaggle/working/preds_preview.csv\n"
     ]
    }
   ],
   "source": [
    "# 11) 导出中间产物与运行信息（日志、版本、校验）\n",
    "\n",
    "def sha256_file(p: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with p.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "meta = {\n",
    "    \"artifact_dataset\": ARTIFACT_DATASET_NAME,\n",
    "    \"artifact_dir\": str(ARTIFACT_DIR),\n",
    "    \"comp_data_dir\": str(COMP_DATA_DIR),\n",
    "    \"model_type\": MODEL_TYPE,\n",
    "    \"median_kernel\": MEDIAN_KERNEL,\n",
    "    \"min_duration_sec\": MIN_DURATION_SEC,\n",
    "    \"num_models\": (len(bundle[\"models\"]) if MODEL_TYPE == \"lgbm\" else None),\n",
    "    \"thresholds_keys\": (len(bundle[\"thresholds\"]) if MODEL_TYPE == \"lgbm\" else None),\n",
    "}\n",
    "\n",
    "# hash a few files for debugging reproducibility\n",
    "hashes = {}\n",
    "th_path = ARTIFACT_DIR / \"thresholds.joblib\"\n",
    "if th_path.exists():\n",
    "    hashes[str(th_path)] = sha256_file(th_path)\n",
    "\n",
    "# hash first 3 model files\n",
    "model_files = sorted((ARTIFACT_DIR / \"models\").glob(\"lgbm_*.txt\"))[:3]\n",
    "for p in model_files:\n",
    "    hashes[str(p)] = sha256_file(p)\n",
    "\n",
    "meta[\"sha256\"] = hashes\n",
    "\n",
    "meta_path = WORK_DIR / \"run_meta.json\"\n",
    "meta_path.write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "preview_path = WORK_DIR / \"preds_preview.csv\"\n",
    "submission.head(100).to_csv(preview_path, index=False)\n",
    "\n",
    "print(\"Wrote:\", meta_path)\n",
    "print(\"Wrote:\", preview_path)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13874099,
     "sourceId": 59156,
     "sourceType": "competition"
    },
    {
     "datasetId": 9009934,
     "sourceId": 14140999,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
