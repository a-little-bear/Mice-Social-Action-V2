experiment_name: "Advanced_HHSTF_Maxed"

seed: 42
debug: false
test: false

# Path Settings
# For Linux/AutoDL, use absolute paths to data disk (e.g., /root/autodl-tmp/...)
data:
  data_dir: "/root/Mice-Social-Action-V2/data/" # Path to the directory containing train.csv and tracking/annotation folders
  batch_size: 2048 # Increased for RTX 6000 96GB.
  num_workers: 16 # Increased for AutoDL high-core count.
  window_size: 64
  preload: true # Enabled! Data is only 2.63GB, safe to load into RAM.
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: false
  cache_size: 8 # Ignored when preload is true.
  
  sampling:
    strategy: "action_rich" # Options: "uniform", "action_rich" (oversample rare classes)
    bias_factor: 5.0

  preprocessing:
    target_fps: 30
    fix_fps: true
    interpolate_nans: true
    unify_body_parts: true
    view: "egocentric" # Options: "allocentric", "egocentric" (mouse-centered)
    
  features:
    enable_strong_features: false
    use_distances: true
    use_velocity: true
    use_acceleration: true
    use_jerk: true 
    use_angles: true
    use_relative_angles: true
    use_body_features: true
    use_window_stats: false
    window_sizes: [5, 15, 30]

  augmentation:
    enabled: true
    flip: true
    rotate: true
    time_stretch: false
    noise: 0.01

model:
  spatial_encoder:
    enabled: true
    type: "gat" # Options: "gat" (Graph Attention), "st_gcn" (Spatio-Temporal GCN), "identity"
    hidden_dim: 128
    num_layers: 3
    dropout: 0.1
    
  temporal_backbone:
    type: "squeezeformer" # Options: "squeezeformer" (Attention+Conv), "wavenet" (Dilated Conv), "transformer", "multi_scale_cnn"
    input_dim: 0 # Will be set dynamically in train.py
    hidden_dim: 512 #256
    num_layers: 8 #6
    kernel_size: 3
    dropout: 0.2
    transformer_heads: 16 #8
    
  fusion:
    type: "attention" # Options: "attention" (Weighted Sum), "concat" (Concatenation), "gated"
    hidden_dim: 1024 #512
    
  classifier:
    num_classes: 76 # Interaction, Investigation, Other
    hidden_dim: 512 #256
    
  context_adapter:
    enabled: true
    embedding_dim: 16

training:
  optimizer: "fused_adamw"
  epochs: 50
  eval_interval: 10
  learning_rate: 0.001
  weight_decay: 0.0001
  scheduler: "cosine" # Options: "cosine", "step", "plateau"
  loss_type: "bce" #"bce" # Options: "focal", "ce", "bce", "ohem", "macro_soft_f1"
  pos_weight: 12.0 # Default weight, overridden if dynamic_pos_weight is true
  dynamic_pos_weight: false # Enabled: Calculate pos_weight per batch based on actual ratio
  ohem_percent: 0.7 # For OHEM loss
  mask_unlabeled: true
  save_dir: "/root/autodl-tmp/checkpoints/" #"checkpoints/" # Directory to save model weights and metrics
  torch_compile: true # Enabled for RTX 6000 to maximize throughput.

cross_validation:
  enabled: false
  n_folds: 5

post_processing:
  n_jobs: 8 # Increased for faster processing with 110GB RAM headroom
  threshold_strategy: "optimize" # Options: "dynamic" (Grid Search), "kaggle" (Hardcoded per-lab), "fixed" (0.5)
  optimize_thresholds: true # Deprecated, kept for backward compatibility (if true and strategy is dynamic, runs grid search)
  tie_breaking: "kaggle" # Options: "none", "z_score", "argmax", "kaggle" (Rule-based)
  smoothing:
    method: "ema" # Options: "median_filter", "ema", "none"
    window_size: 7
    alpha: 0.3 # For EMA
  gap_filling:
    max_gap: 10
    min_duration: 5


